# -*- coding: utf-8 -*-
"""LWIR_to_RGB

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qTQDyp1AsFLwQwOsxgLMlaDqtp_ZzRud

#Import Libraries and Mount Drive
"""

import os
import sys
import shutil 
import glob

import numpy as np
import random
import tensorflow as tf
from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
import cv2 as cv


import torch
import torchvision
from time import time
from torchvision import datasets, transforms
from torchvision.io import read_image
from torch import nn, optim
from torch.utils.data import Dataset
print(torch.__version__)

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

"""#Create Directories to train the model"""

parent_dir_j = "/content/drive/MyDrive/In-Bed-Human-Pose-Estimation(VIP-CUP)"
parent_dir = "/content/drive/MyDrive/LWIR_to_RGB"
#os.mkdir(parent_dir)

cd '/content/drive/MyDrive/LWIR_to_RGB'

!ls '/content/drive/MyDrive/LWIR_to_RGB'

dirc = "A"
path_lwir = os.path.join(parent_dir,dirc)
#os.mkdir(path_lwir)

dirc = "B"
path_rgb = os.path.join(parent_dir,dirc)
#os.mkdir(path_rgb)

path_lwir_train = os.path.join(path_lwir,"train" )
#os.mkdir(path_lwir_train)

path_lwir_val = os.path.join(path_lwir, "val")
#os.mkdir(path_lwir_val)

path_lwir_test = os.path.join(path_lwir, "test")
#os.mkdir(path_lwir_test)

path_rgb_train = os.path.join(path_rgb, "train")
#os.mkdir(path_rgb_train)

path_rgb_val = os.path.join(path_rgb, "val")
#os.mkdir(path_rgb_val)

path_rgb_test = os.path.join(path_rgb, "test")
#os.mkdir(path_rgb_test)

"""#Transfer images from dataset to train, val, test in A (LWIR) and B (RGB)

"""

# Transfering uncovered images for train and val data

cover_type = 'uncover'
count = 1
for i in range(1,21):
  if i < 10:
      data_name = "0000"+str(i)              
  else:
      data_name = "000"+str(i) 
  
  #Destination path
  if i<=15: 
    destination_lwir = path_lwir_train
    destination_rgb = path_rgb_train
    if i==15:
      print("Moving last subject in Training")
  elif (i>15 and i<=20):
    destination_lwir = path_lwir_val
    destination_rgb = path_rgb_val
    if i==20:
      print("Moving last subject in Val")

  for j in range(1,46):
    if j < 10:
        img_name = "00000"+str(j)
    else:
        img_name = "0000"+str(j)  


    #Source LWIR
    source = '/content/drive/MyDrive/In-Bed-Human-Pose-Estimation(VIP-CUP)/train/' + data_name +'/IR/'+cover_type+'/image_'+img_name +'.png'
    #im_ir = cv.imread(source)
    #im_ir = cv.resize(im_ir, (576, 1024), interpolation = cv.INTER_AREA)
    #print(im_ir.shape)
    #cv.imwrite(os.path.join(destination_lwir,str(count)+'.png' ), im_ir)
    shutil.copy(source, destination_lwir)
    os.rename(os.path.join(destination_lwir,'image_'+img_name +'.png' ),os.path.join(destination_lwir,str(count)+'.png' ))
    #count+=1

    #Source RGB
    source = '/content/drive/MyDrive/In-Bed-Human-Pose-Estimation(VIP-CUP)/train/' + data_name +'/RGB/'+cover_type+'/image_'+img_name +'.png'
    im_rgb = cv.imread(source)
    im_rgb = cv.resize(im_rgb, (120,160), interpolation = cv.INTER_AREA)
    #print(im_rgb.shape)
    cv.imwrite(os.path.join(destination_rgb,str(count)+'.png' ), im_rgb)
    #shutil.copy(source, destination_rgb)
    #os.rename(os.path.join(destination_rgb,'image_'+img_name +'.png' ),os.path.join(destination_rgb,str(count)+'.png' ))
    count+=1
    #break
  #break

# Transfering covered images for test data

cover_type = 'cover2'    #change this for thin cover => "cover1" and thick cover => "cover2"
#count = 1
for i in range(71,73):      # any subjects in range 31-55 for thin cover and 56-80 for thick cover 
  if i < 10:
      data_name = "0000"+str(i)              
  else:
      data_name = "000"+str(i) 
  
  #Destination path

  
  destination_lwir = path_lwir_test
  destination_rgb = path_rgb_test 
  if i==32:
    print("Moving last subject in Testing")

  for j in range(1,46):
    if j < 10:
        img_name = "00000"+str(j)
    else:
        img_name = "0000"+str(j)  


    #Source LWIR
    source = '/content/drive/MyDrive/In-Bed-Human-Pose-Estimation(VIP-CUP)/train/' + data_name +'/IR/'+cover_type+'/image_'+img_name +'.png'
    #im_ir = cv.imread(source)
    #im_ir = cv.resize(im_ir, (576, 1024), interpolation = cv.INTER_AREA)
    #print(im_ir.shape)
    #cv.imwrite(os.path.join(destination_lwir,str(count)+'.png' ), im_ir)
    shutil.copy(source, destination_lwir)
    os.rename(os.path.join(destination_lwir,'image_'+img_name +'.png' ),os.path.join(destination_lwir,str(count)+'.png' ))
    #count+=1

    #Source RGB
    source = '/content/drive/MyDrive/In-Bed-Human-Pose-Estimation(VIP-CUP)/train/' + data_name +'/RGB/'+cover_type+'/image_'+img_name +'.png'
    im_rgb = cv.imread(source)
    im_rgb = cv.resize(im_rgb, (120,160), interpolation = cv.INTER_AREA)
    #print(im_rgb.shape)
    cv.imwrite(os.path.join(destination_rgb,str(count)+'.png' ), im_rgb)
    #shutil.copy(source, destination_rgb)
    #os.rename(os.path.join(destination_rgb,'image_'+img_name +'.png' ),os.path.join(destination_rgb,str(count)+'.png' ))
    count+=1
    #break
  #break

"""# Install 
 from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix
"""

!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix

import os
os.chdir('pytorch-CycleGAN-and-pix2pix/')

!pip install -r requirements.txt

"""# Datasets

Download one of the official datasets with:

-   `bash ./datasets/download_pix2pix_dataset.sh [cityscapes, night2day, edges2handbags, edges2shoes, facades, maps]`

Or use your own dataset by creating the appropriate folders and adding in the images. Follow the instructions [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/datasets.md#pix2pix-datasets).
"""

#!bash ./datasets/download_pix2pix_dataset.sh facades

cd '/content/drive/MyDrive/LWIR_to_RGB/pytorch-CycleGAN-and-pix2pix'

"""Run this to create dataset used uncovered LWIR and RGB pairs in training and Validation, and covered pairs in test"""

!python datasets/combine_A_and_B.py --fold_A /content/drive/MyDrive/LWIR_to_RGB/A  --fold_B /content/drive/MyDrive/LWIR_to_RGB/B --fold_AB /content/drive/MyDrive/LWIR_to_RGB

"""#Training Pix2Pix"""

cd '/content/drive/MyDrive/LWIR_to_RGB'

!python /content/drive/MyDrive/LWIR_to_RGB/pytorch-CycleGAN-and-pix2pix/train.py --dataroot /content/drive/MyDrive/LWIR_to_RGB --name lwir_rgb_jat --model pix2pix --direction AtoB  --save_epoch_freq 400 --n_epochs 200 --n_epochs_decay 200

ls '/content/drive/MyDrive/LWIR_to_RGB/checkpoints/lwir_rgb_jat/'

"""#Testing on trained model using covered images

"""

!python /content/drive/MyDrive/LWIR_to_RGB/pytorch-CycleGAN-and-pix2pix/test.py --dataroot /content/drive/MyDrive/LWIR_to_RGB  --direction AtoB --model pix2pix --name lwir_rgb_jat --num_test 180

"""# Not necessary but kept for reference

## Training

-   `python train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA`

Change the `--dataroot` and `--name` to your own dataset's path and model's name. Use `--gpu_ids 0,1,..` to train on multiple GPUs and `--batch_size` to change the batch size. Add `--direction BtoA` if you want to train a model to transfrom from class B to A.

## Testing

-   `python test.py --dataroot ./datasets/facades --direction BtoA --model pix2pix --name facades_pix2pix`

Change the `--dataroot`, `--name`, and `--direction` to be consistent with your trained model's configuration and how you want to transform images.

> from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix:
> Note that we specified --direction BtoA as Facades dataset's A to B direction is photos to labels.

> If you would like to apply a pre-trained model to a collection of input images (rather than image pairs), please use --model test option. See ./scripts/test_single.sh for how to apply a model to Facade label maps (stored in the directory facades/testB).

> See a list of currently available models at ./scripts/download_pix2pix_model.sh

## Visualize
"""

import matplotlib.pyplot as plt

img = plt.imread('./results/facades_label2photo_pretrained/test_latest/images/100_fake_B.png')
plt.imshow(img)

img = plt.imread('./results/facades_label2photo_pretrained/test_latest/images/100_real_A.png')
plt.imshow(img)

img = plt.imread('./results/facades_label2photo_pretrained/test_latest/images/100_real_B.png')
plt.imshow(img)

"""## Pretrained models

Download one of the official pretrained models with:

-   `bash ./scripts/download_pix2pix_model.sh [edges2shoes, sat2map, map2sat, facades_label2photo, and day2night]`

Or add your own pretrained model to `./checkpoints/{NAME}_pretrained/latest_net_G.pt`
"""

!bash ./scripts/download_pix2pix_model.sh facades_label2photo

!python train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA