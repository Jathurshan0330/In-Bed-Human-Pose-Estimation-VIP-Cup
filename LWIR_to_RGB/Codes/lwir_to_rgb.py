# -*- coding: utf-8 -*-
"""LWIR_to_RGB

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qTQDyp1AsFLwQwOsxgLMlaDqtp_ZzRud

#Import Libraries and Mount Drive
"""

import os
import sys
import shutil 
import glob

import numpy as np
import random
# import tensorflow as tf
# from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt
# import matplotlib.gridspec as gridspec
import cv2 as cv
from skimage import transform
import skimage.io as io
from skimage.transform import resize
from skimage import transform
from skimage import img_as_ubyte

import torch
import torchvision
from time import time
from torchvision import datasets, transforms
from torchvision.io import read_image
from torch import nn, optim
from torch.utils.data import Dataset
print(torch.__version__)

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

"""#Create Directories to train the model"""

parent_dir_j = "/content/drive/MyDrive/In-Bed-Human-Pose-Estimation(VIP-CUP)"
parent_dir = "/content/drive/MyDrive/LWIR_to_RGB"
#os.mkdir(parent_dir)

cd '/content/drive/MyDrive/LWIR_to_RGB'

!ls '/content/drive/MyDrive/LWIR_to_RGB'

dirc = "A"
path_lwir = os.path.join(parent_dir,dirc)


dirc = "B"
path_rgb = os.path.join(parent_dir,dirc)

# os.mkdir(path_lwir)
# os.mkdir(path_rgb)

path_lwir_train = os.path.join(path_lwir,"train" )
#os.mkdir(path_lwir_train)

path_lwir_val = os.path.join(path_lwir, "val")
#os.mkdir(path_lwir_val)

path_lwir_test = os.path.join(path_lwir, "test")
#os.mkdir(path_lwir_test)

path_rgb_train = os.path.join(path_rgb, "train")
#os.mkdir(path_rgb_train)

path_rgb_val = os.path.join(path_rgb, "val")
#os.mkdir(path_rgb_val)

path_rgb_test = os.path.join(path_rgb, "test")
#os.mkdir(path_rgb_test)

"""#Transfer images from dataset to train, val, test in A (LWIR) and B (RGB)

"""

def get_data(source_path,destination_path, cover_type,start_sub,end_sub,count = 1):
   
  #count = 1
  for i in range(start_sub,end_sub+1):
    if i < 10:
        data_name = "0000"+str(i)              
    else:
        data_name = "000"+str(i) 
    print("Getting Data from : {}".format(data_name))
    #Destination path
    if i<=15: 
      destination_lwir = os.path.join(destination_path,'A',"train" )
      destination_rgb = os.path.join(destination_path,'B',"train" )
      if i==15:
        print("Moving last subject in Training")
    elif (i>15 and i<=20):
      destination_lwir = os.path.join(destination_path,'A',"val" )
      destination_rgb = os.path.join(destination_path,'B',"val" )
      if i==20:
        print("Moving last subject in Val")
    else:
      destination_lwir = os.path.join(destination_path,'A',"test" )
      destination_rgb = os.path.join(destination_path,'B',"test" )

    data_path =  os.path.join(source_path, data_name) 

    # get homography transformation matrices
    H_rgb = np.array(np.load(os.path.join(data_path, "align_PTr_RGB.npy")))
    H_ir = np.array(np.load(os.path.join(data_path, "align_PTr_IR.npy")))
    H = np.matmul(np.linalg.inv(H_rgb), H_ir) # RGB -> IR tarnsformation
    tform = transform.ProjectiveTransform(matrix=H)


    for img_name in os.listdir(os.path.join(data_path,'RGB',cover_type)):
      if img_name.endswith('png'):
        
        #Source LWIR
        source = data_path  +'/IR/'+cover_type+'/'+img_name 
        shutil.copy(source, destination_lwir)
        os.rename(os.path.join(destination_lwir,img_name ),os.path.join(destination_lwir,str(count)+'.png' ))


        #Source RGB
        source = data_path  +'/RGB/'+cover_type+'/'+img_name 
        im_rgb = io.imread(source)
        im_rgb = transform.warp(im_rgb, tform)
        im_rgb = im_rgb[0:160, 0:120]
        io.imsave(os.path.join(destination_rgb,str(count)+'.png' ), img_as_ubyte(im_rgb))
        count+=1
        
  return count

source_path = "/content/drive/MyDrive/In-Bed-Human-Pose-Estimation(VIP-CUP)/train/"
destination_path = "/content/drive/MyDrive/LWIR_to_RGB"
#get_data(source_path,destination_path, 'uncover',1,20)  ## Transfering uncovered images for train and val data
gtest_count = get_data(source_path,destination_path, 'cover1',35,37)  # Transfering thin covered images for test data
get_data(source_path,destination_path, 'cover2',76,77,gtest_count )  # Transfering thick covered images for test data

"""# Install 
 from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix
"""

!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix

import os
os.chdir('pytorch-CycleGAN-and-pix2pix/')

!pip install -r requirements.txt

"""# Create Datasets

Download one of the official datasets with:

-   `bash ./datasets/download_pix2pix_dataset.sh [cityscapes, night2day, edges2handbags, edges2shoes, facades, maps]`

Or use your own dataset by creating the appropriate folders and adding in the images. Follow the instructions [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/datasets.md#pix2pix-datasets).
"""

cd '/content/drive/MyDrive/LWIR_to_RGB/pytorch-CycleGAN-and-pix2pix'

"""Run this to create dataset used uncovered LWIR and RGB pairs in training and Validation, and covered pairs in test"""

!python datasets/combine_A_and_B.py --fold_A /content/drive/MyDrive/LWIR_to_RGB/A  --fold_B /content/drive/MyDrive/LWIR_to_RGB/B --fold_AB /content/drive/MyDrive/LWIR_to_RGB

"""#Training Pix2Pix"""

cd '/content/drive/MyDrive/LWIR_to_RGB'

!python /content/drive/MyDrive/LWIR_to_RGB/pytorch-CycleGAN-and-pix2pix/train.py --dataroot /content/drive/MyDrive/LWIR_to_RGB --name lwir_rgb_jat_3 --model pix2pix --direction AtoB  --save_epoch_freq 200 --n_epochs 100 --n_epochs_decay 100

ls '/content/drive/MyDrive/LWIR_to_RGB/checkpoints/lwir_rgb_jat_3/'

"""#Testing on trained model using covered images

"""

!python /content/drive/MyDrive/LWIR_to_RGB/pytorch-CycleGAN-and-pix2pix/test.py --dataroot /content/drive/MyDrive/LWIR_to_RGB  --direction AtoB --model pix2pix --name lwir_rgb_jat_3 --num_test 180

"""# Not necessary but kept for reference

## Training

-   `python train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA`

Change the `--dataroot` and `--name` to your own dataset's path and model's name. Use `--gpu_ids 0,1,..` to train on multiple GPUs and `--batch_size` to change the batch size. Add `--direction BtoA` if you want to train a model to transfrom from class B to A.

## Testing

-   `python test.py --dataroot ./datasets/facades --direction BtoA --model pix2pix --name facades_pix2pix`

Change the `--dataroot`, `--name`, and `--direction` to be consistent with your trained model's configuration and how you want to transform images.

> from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix:
> Note that we specified --direction BtoA as Facades dataset's A to B direction is photos to labels.

> If you would like to apply a pre-trained model to a collection of input images (rather than image pairs), please use --model test option. See ./scripts/test_single.sh for how to apply a model to Facade label maps (stored in the directory facades/testB).

> See a list of currently available models at ./scripts/download_pix2pix_model.sh

## Visualize
"""

import matplotlib.pyplot as plt

img = plt.imread('./results/facades_label2photo_pretrained/test_latest/images/100_fake_B.png')
plt.imshow(img)

img = plt.imread('./results/facades_label2photo_pretrained/test_latest/images/100_real_A.png')
plt.imshow(img)

img = plt.imread('./results/facades_label2photo_pretrained/test_latest/images/100_real_B.png')
plt.imshow(img)

"""## Pretrained models

Download one of the official pretrained models with:

-   `bash ./scripts/download_pix2pix_model.sh [edges2shoes, sat2map, map2sat, facades_label2photo, and day2night]`

Or add your own pretrained model to `./checkpoints/{NAME}_pretrained/latest_net_G.pt`
"""

!bash ./scripts/download_pix2pix_model.sh facades_label2photo

!python train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA