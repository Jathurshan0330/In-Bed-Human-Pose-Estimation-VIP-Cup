{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TRAIN_unpaired_img2img_translation.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"rLJ9sejAcMNk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626760850896,"user_tz":-330,"elapsed":28570,"user":{"displayName":"Udith Haputhanthri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdAtVVmDFOeesHiPyslaX9NW9ijHN3b6d888ZX6ks=s64","userId":"03629779488105059696"}},"outputId":"6e2bc6fa-a653-41fa-a7d2-00400be6b615"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive') #inbedpose\n","!cp -r '/content/gdrive/MyDrive/In-Bed-Human-Pose-Estimation(VIP-CUP)' /content"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6ZpBSqDCcln8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626760853910,"user_tz":-330,"elapsed":3020,"user":{"displayName":"Udith Haputhanthri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdAtVVmDFOeesHiPyslaX9NW9ijHN3b6d888ZX6ks=s64","userId":"03629779488105059696"}},"outputId":"0e3ca582-d4b2-4e1b-ff49-53d0789bf2a0"},"source":["!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\n","!git clone https://github.com/taesungp/contrastive-unpaired-translation CUT"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'pytorch-CycleGAN-and-pix2pix'...\n","remote: Enumerating objects: 2340, done.\u001b[K\n","remote: Counting objects: 100% (3/3), done.\u001b[K\n","remote: Compressing objects: 100% (3/3), done.\u001b[K\n","remote: Total 2340 (delta 0), reused 1 (delta 0), pack-reused 2337\u001b[K\n","Receiving objects: 100% (2340/2340), 8.10 MiB | 25.51 MiB/s, done.\n","Resolving deltas: 100% (1499/1499), done.\n","Cloning into 'CUT'...\n","remote: Enumerating objects: 256, done.\u001b[K\n","remote: Counting objects: 100% (58/58), done.\u001b[K\n","remote: Compressing objects: 100% (33/33), done.\u001b[K\n","remote: Total 256 (delta 37), reused 25 (delta 25), pack-reused 198\u001b[K\n","Receiving objects: 100% (256/256), 17.90 MiB | 33.21 MiB/s, done.\n","Resolving deltas: 100% (128/128), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-IdtKGpCgiv1"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qsjEFjBUctEf"},"source":["import glob\n","import shutil, os\n","\n","def copy_imgs(img_dirs, target_dir, A_or_B='A', train_test_ratio= 0.8):\n","  n_images= len(img_dirs)\n","  n_train= int(n_images*0.8)\n","  n_test= n_images- n_train\n","\n","  if os.path.isdir(f'{target_dir}/train{A_or_B}'):shutil.rmtree(f'{target_dir}/train{A_or_B}')\n","  if os.path.isdir(f'{target_dir}/test{A_or_B}'):shutil.rmtree( f'{target_dir}/test{A_or_B}')\n","\n","  os.makedirs(f'{target_dir}/train{A_or_B}')\n","  os.makedirs(f'{target_dir}/test{A_or_B}')\n","\n","  for i in range(n_images):\n","    img_dir= img_dirs[i]\n","    img_name= (6-len(str(i+1)))*'0'+str(i+1)+'.jpg'\n","    if i<n_train:target_img_dir= f'{target_dir}/train{A_or_B}/{img_name}'\n","    else:target_img_dir= f'{target_dir}/test{A_or_B}/{img_name}'\n","    \n","    print(target_img_dir)\n","    shutil.copyfile(img_dir, target_img_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VYpfEnBRfC5K"},"source":["uncover_img_dirs = sorted(glob.glob('/content/In-Bed-Human-Pose-Estimation(VIP-CUP)/train/*/IR/uncover/*'))\n","cover1_img_dirs = sorted(glob.glob('/content/In-Bed-Human-Pose-Estimation(VIP-CUP)/train/*/IR/cover1/*'))\n","cover2_img_dirs = sorted(glob.glob('/content/In-Bed-Human-Pose-Estimation(VIP-CUP)/train/*/IR/cover2/*'))\n","\n","copy_imgs(cover1_img_dirs, 'datasets/inbed', 'A', 0.8)\n","copy_imgs(uncover_img_dirs, 'datasets/inbed', 'B', 0.8)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TKVXDGbLdA9G"},"source":["%%capture\n","!pip install -r pytorch-CycleGAN-and-pix2pix/requirements.txt\n","!pip install -r CUT/requirements.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EV76PdE_eNla"},"source":["# checkpoint dir: put drive link if needed\n","# ignore visualization errors due to localhosts\n","\n","#!python /content/pytorch-CycleGAN-and-pix2pix/train.py --dataroot /content/datasets/inbed --name InbedPose_CyleGAN --model cycle_gan --checkpoints_dir '/content/checkpoints' --save_epoch_freq 1\n","#!python /content/CUT/train.py --dataroot /content/datasets/inbed --name InbedPose_CUT --CUT_mode CUT --checkpoints_dir '/content/checkpoints' --save_epoch_freq 1\n","#!python /content/CUT/train.py --dataroot /content/datasets/inbed --name InbedPose_FastCUT --CUT_mode FastCUT --checkpoints_dir '/content/checkpoints' --save_epoch_freq 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b_JbiqyOlR0N"},"source":[""],"execution_count":null,"outputs":[]}]}